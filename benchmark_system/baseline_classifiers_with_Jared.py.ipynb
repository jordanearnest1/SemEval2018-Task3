{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd  ##going to use read csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep/Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"SemEval2018-T3-train-taskA.txt\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Sweet United Nations video. Just in time for C...\n",
       "1       @mrdahl87 We are rumored to have talked to Erv...\n",
       "2       Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3                     3 episodes left I'm dying over here\n",
       "4       I can't breathe! was chosen as the most notabl...\n",
       "5       You're never too old for Footie Pajamas. http:...\n",
       "6       Nothing makes me happier then getting on the h...\n",
       "7       4:30 an opening my first beer now gonna be a l...\n",
       "8       @Adam_Klug do you think you would support a gu...\n",
       "9       @samcguigan544 You are not allowed to open tha...\n",
       "10      Oh, thank GOD - our entire office email system...\n",
       "11      But instead, I'm scrolling through Facebook, I...\n",
       "12      @TargetZonePT :pouting_face: no he bloody isn'...\n",
       "13      Cold or warmth both suffuse one's cheeks with ...\n",
       "14      Just great when you're mobile bill arrives by ...\n",
       "15      crushes are great until you realize they'll ne...\n",
       "16      Buffalo sports media is smarter than all of us...\n",
       "17      I guess my cat also lost 3 pounds when she wen...\n",
       "18      @YankeesWFAN @Ken_Rosenthal trading a SP for a...\n",
       "19      But @DarklightDave was trying to find us, and ...\n",
       "20      @deputymartinski please do..i need the second ...\n",
       "21      I never cared for Beyonce, bc I could never ge...\n",
       "22                 @yWTorres9 time to hit the books then \n",
       "23      @RushOrderTees THX4FLW! FLWtheMUSIC @ElektrikE...\n",
       "24      Love these cold winter mornings :grimacing_fac...\n",
       "25      Amazingly http://t.co/NEioZuNbLD is not owned ...\n",
       "26      Wish she could have told me herself. @NicoleSc...\n",
       "27      The rain has made extra extra lazy:smiling_fac...\n",
       "28      I was doing great with this summary of my year...\n",
       "29      see, that MIGHT show up on a background check,...\n",
       "                              ...                        \n",
       "3787    @Nylons @quick13 @jamieyuccas @chadhartman it ...\n",
       "3788    @TheFollowingFOX I get paid 4 posting stuff li...\n",
       "3789    @Abelv03 @KWAPT I just want learning from this...\n",
       "3790    Only ones in the cinema  #putting #my #phone #...\n",
       "3791    @BBCRadMac @StuartMaconie years ago in M && S ...\n",
       "3792    Montana Of 300 14 - THE BEST Versace Remix IN ...\n",
       "3793    I should of just made a canvas of coffee stain...\n",
       "3794    The world is such a smiley place. :flushed_face: \n",
       "3795    Two Broke Rednecks father/daughter riffing tea...\n",
       "3796    #WTF is happening to these kids??? Are you kid...\n",
       "3797    I would have made a much more convincing Bella...\n",
       "3798    I retweeted this so Chris Graham blocked me. |...\n",
       "3799              Fries With That 304 #AlabamaStateMajors\n",
       "3800    @StartUpGrindBuf @magnachef If you need a #dev...\n",
       "3801    I'm glad the DC Council has it's priorities in...\n",
       "3802            Riding the distraction train. ||CHOO CHOO\n",
       "3803    Chill #Repost #Dead  #Dominos #Haha :face_with...\n",
       "3804    Someone I work w doesn't let his kids believe ...\n",
       "3805    Check out my new post! MyFairDaily: 10 Things ...\n",
       "3806    Obama whisked away to hospital, diagnosed with...\n",
       "3807    @DCsportsGrl @DragonflyJonez true n that's y w...\n",
       "3808    Another one of our support vehicles modified f...\n",
       "3809                 Thanks for shutting the city down.. \n",
       "3810    @flippysgardenia IKR?! don't you see? he's gon...\n",
       "3811    Glad there's not a typhoon where we go on holi...\n",
       "3812    @banditelli regarding what the PSU president does\n",
       "3813    @banditelli But still bothers me that I see no...\n",
       "3814    well now that i've listened to all of into the...\n",
       "3815    Hummingbirds #Are  #Experts #at #Hovering #Aft...\n",
       "3816    Only thing missing now is a session at the gym...\n",
       "Name: Tweet text, Length: 3817, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"Tweet text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## to get a list of strings to feed into, put brackets after data frame name and get a list out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-96cb4c2105e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweet text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Fix HTML character entities:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_replace_html_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;31m# Remove username handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_handles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/casual.py\u001b[0m in \u001b[0;36m_replace_html_entities\u001b[0;34m(text, keep, remove_illegal, encoding)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mremove_illegal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mENT_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_entity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_str_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tokenizer.tokenize(tweets_df[\"Tweet text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweet in tweets_df[\"Tweet text\"]:\n",
    "#     tokenizer.tokenize(tweet)  ##creates a list of tokens for each string (each tweet)\n",
    "\n",
    "tokenize_tweets = [tokenizer.tokenize(tweet) for tweet in tweets_df[\"Tweet text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sweet',\n",
       " 'united',\n",
       " 'nations',\n",
       " 'video',\n",
       " '.',\n",
       " 'just',\n",
       " 'in',\n",
       " 'time',\n",
       " 'for',\n",
       " 'christmas',\n",
       " '.',\n",
       " '#imagine',\n",
       " '#noreligion',\n",
       " 'http://t.co/fej2v3OUBR']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## term-document matrix : does this term occur in this document \n",
    "# The red fox. \n",
    "# The green frog.\n",
    "\n",
    "# The red green frog fox\n",
    "# [1, 1, 0, 0, 1]\n",
    "# [1, 0 , 1, 1, 0]\n",
    "\n",
    "## matrix is the width of every term that appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_strings = []\n",
    "for lst in tokenize_tweets:\n",
    "    new_string = \"\"\n",
    "    for word in lst:\n",
    "        new_string += word + \" \"\n",
    "    lst_of_strings.append(new_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sweet united nations video . just in time for christmas . #imagine #noreligion http://t.co/fej2v3OUBR ',\n",
       " \"we are rumored to have talked to erv's agent ... and the angels asked about ed escobar ... that's hardly nothing ;) \",\n",
       " 'hey there ! nice to see you minnesota / nd winter weather ',\n",
       " \"3 episodes left i'm dying over here \",\n",
       " \"i can't breathe ! was chosen as the most notable quote of the year in an annual list released by a yale university librarian \"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_of_strings[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer()\n",
    "# doc_term = cv.fit_transform(lst_of_strings)\n",
    "# doc_term = doc_term.todense()  ## takes our sparse matrix and makes it dense\n",
    "\n",
    "# implementing n-grams\n",
    "# cv = CountVectorizer(ngram_range=(1,2)) ## counting everyword and pair of words\n",
    "# doc_term = cv.fit_transform(lst_of_strings)\n",
    "# doc_term = doc_term.todense()  ## takes our sparse matrix and makes it dense\n",
    "\n",
    "##now trying pairs of words\n",
    "# cv = CountVectorizer(ngram_range=(2,2)) \n",
    "# doc_term = cv.fit_transform(lst_of_strings)\n",
    "# doc_term = doc_term.todense()  ## this result was worse\n",
    "\n",
    "\n",
    "#now trying single words and 3-grams \n",
    "cv = CountVectorizer(ngram_range=(1,3)) \n",
    "doc_term = cv.fit_transform(lst_of_strings)\n",
    "doc_term = doc_term.todense()  ## this result was better, spiked up faster at the beginning\n",
    "\n",
    "# ##now trying single words and 5-grams \n",
    "# cv = CountVectorizer(ngram_range=(1,5)) \n",
    "# doc_term = cv.fit_transform(lst_of_strings)\n",
    "# doc_term = doc_term.todense()  ## maybe too far\n",
    "\n",
    "##we just did a \"grid search\" -- trying different things to see what works best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## vocab = 10770 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB    ##these are model objects\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "\n",
    "## Jared prefers LR over NB because NB has been used for very specific things. \n",
    "#For general classification, LR works better. \n",
    "# \"No Free Lunch Theorem\" states that there is not model that's the best model for everything\n",
    "## not every sklearn model is able to take sparse matrices in \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "## library for model selection to get train vs test set\n",
    "## it wants my x matrix and y vector \n",
    "\n",
    "## will return 4 arrays: labels for training set and the test set. 75% training, 25% test.\n",
    "#(x's are terms, y's are labels)\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(doc_term, tweets_df[\"Label\"]) \n",
    "## on all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3817"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()  \n",
    "\n",
    "## RF is an ensemble of dumb classifiers, \n",
    "#makes a bunch of decision trees that are trained on some random columns.\n",
    "# grab random columns, make a decision tree, and keep repeating. then do a decision tree with them in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] .................................. C=1, penalty=l1, total=   4.9s\n",
      "[CV] C=1, penalty=l1 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................. C=1, penalty=l1, total=   4.3s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] .................................. C=1, penalty=l1, total=   4.1s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] .................................. C=1, penalty=l1, total=   4.2s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] .................................. C=1, penalty=l1, total=   4.1s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] .................................. C=1, penalty=l2, total=   3.9s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] .................................. C=1, penalty=l2, total=   3.8s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] .................................. C=1, penalty=l2, total=   3.8s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] .................................. C=1, penalty=l2, total=   3.8s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] .................................. C=1, penalty=l2, total=   3.9s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ................................. C=10, penalty=l1, total=   4.1s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ................................. C=10, penalty=l1, total=   4.1s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ................................. C=10, penalty=l1, total=   3.9s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ................................. C=10, penalty=l1, total=   4.1s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ................................. C=10, penalty=l1, total=   4.2s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ................................. C=10, penalty=l2, total=   4.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ................................. C=10, penalty=l2, total=   4.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ................................. C=10, penalty=l2, total=   4.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ................................. C=10, penalty=l2, total=   3.8s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ................................. C=10, penalty=l2, total=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ('l1', 'l2'), 'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr = LogisticRegression(penalty='l1') --> didn't help\n",
    "# lr = LogisticRegression(C=.05) -> not as bad as l1 on its own)\n",
    "# lr = LogisticRegression(penalty=\"l1\", C=.05) --> the worst ever\n",
    "\n",
    "#should continue to read about cross-validation \n",
    "# (re: splitting stratetgies, what's the best way to split your data. \n",
    "#you have a bunch of true and false labeled data, \n",
    "#is it important you maintain the same level of true/false in each split?)\n",
    "#jared likes shuffle_split\n",
    "\n",
    "##regularization (penalization) sets up a wall to avoid overfitting. \n",
    "# it's still possible to overfit, with strong evidence\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':[1, 10]}\n",
    "clf = GridSearchCV(lr, parameters, cv=5, verbose=2)\n",
    "clf.fit(doc_term, tweets_df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.886750</td>\n",
       "      <td>0.427994</td>\n",
       "      <td>0.627980</td>\n",
       "      <td>0.852568</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.854849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606815</td>\n",
       "      <td>0.853307</td>\n",
       "      <td>0.647444</td>\n",
       "      <td>0.853962</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>0.852980</td>\n",
       "      <td>0.293852</td>\n",
       "      <td>0.011685</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.002497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.436145</td>\n",
       "      <td>0.395159</td>\n",
       "      <td>0.642389</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.669281</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.672346</td>\n",
       "      <td>0.999018</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.024790</td>\n",
       "      <td>0.010144</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.657279</td>\n",
       "      <td>0.421912</td>\n",
       "      <td>0.619073</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>10</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.011712</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.556490</td>\n",
       "      <td>0.405126</td>\n",
       "      <td>0.633482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.661438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080161</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0       3.886750         0.427994         0.627980          0.852568       1   \n",
       "1       3.436145         0.395159         0.642389          0.999542       1   \n",
       "2       3.657279         0.421912         0.619073          0.999803      10   \n",
       "3       3.556490         0.405126         0.633482          1.000000      10   \n",
       "\n",
       "  param_penalty                      params  rank_test_score  \\\n",
       "0            l1   {'C': 1, 'penalty': 'l1'}                3   \n",
       "1            l2   {'C': 1, 'penalty': 'l2'}                1   \n",
       "2            l1  {'C': 10, 'penalty': 'l1'}                4   \n",
       "3            l2  {'C': 10, 'penalty': 'l2'}                2   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "0           0.662745            0.854849       ...                  0.606815   \n",
       "1           0.669281            0.999672       ...                  0.626474   \n",
       "2           0.622222            0.999672       ...                  0.604194   \n",
       "3           0.661438            1.000000       ...                  0.618611   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.853307           0.647444            0.853962   \n",
       "1            1.000000           0.672346            0.999018   \n",
       "2            1.000000           0.643512            1.000000   \n",
       "3            1.000000           0.661861            1.000000   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.597641            0.852980      0.293852        0.011685   \n",
       "1           0.605505            0.999673      0.024790        0.010144   \n",
       "2           0.605505            0.999345      0.106263        0.011712   \n",
       "3           0.604194            1.000000      0.080161        0.015357   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.024353         0.002497  \n",
       "1        0.025501         0.000334  \n",
       "2        0.014233         0.000262  \n",
       "3        0.023734         0.000000  \n",
       "\n",
       "[4 rows x 22 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_\n",
    "pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "# training score is the results of putting training data into fitted model and seeing what prediction is\n",
    "# tells you how well you predict data you've already seen \n",
    "# test score / training score discrepancy means overfitted! \n",
    "# an overfitted model could be the best model, but still have to mention it's overfitting\n",
    "## want the test and training numbers to be closer to gether. restricting depth is one method of doing so with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### best_params were: C= 1 and Penalty = l2\n",
    "# the default cv was 3, we increased it to 5. \n",
    "#LR is a simple model, RF has more params to change grid search on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV] max_depth=6, n_estimators=32 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=32, total=   6.1s\n",
      "[CV] max_depth=6, n_estimators=32 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... max_depth=6, n_estimators=32, total=   4.6s\n",
      "[CV] max_depth=6, n_estimators=64 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=64, total=   6.1s\n",
      "[CV] max_depth=6, n_estimators=64 ....................................\n",
      "[CV] ..................... max_depth=6, n_estimators=64, total=   5.9s\n",
      "[CV] max_depth=6, n_estimators=128 ...................................\n",
      "[CV] .................... max_depth=6, n_estimators=128, total=   8.9s\n",
      "[CV] max_depth=6, n_estimators=128 ...................................\n",
      "[CV] .................... max_depth=6, n_estimators=128, total=   8.8s\n",
      "[CV] max_depth=8, n_estimators=32 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=32, total=   5.0s\n",
      "[CV] max_depth=8, n_estimators=32 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=32, total=   4.9s\n",
      "[CV] max_depth=8, n_estimators=64 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=64, total=   6.8s\n",
      "[CV] max_depth=8, n_estimators=64 ....................................\n",
      "[CV] ..................... max_depth=8, n_estimators=64, total=   7.2s\n",
      "[CV] max_depth=8, n_estimators=128 ...................................\n",
      "[CV] .................... max_depth=8, n_estimators=128, total=  11.1s\n",
      "[CV] max_depth=8, n_estimators=128 ...................................\n",
      "[CV] .................... max_depth=8, n_estimators=128, total=  11.3s\n",
      "[CV] max_depth=10, n_estimators=32 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=32, total=   5.4s\n",
      "[CV] max_depth=10, n_estimators=32 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=32, total=   5.3s\n",
      "[CV] max_depth=10, n_estimators=64 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=64, total=   7.6s\n",
      "[CV] max_depth=10, n_estimators=64 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=64, total=   7.6s\n",
      "[CV] max_depth=10, n_estimators=128 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=128, total=  12.3s\n",
      "[CV] max_depth=10, n_estimators=128 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=128, total=  12.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [32, 64, 128], 'max_depth': [6, 8, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## running grid search on RF\n",
    "parameters = {'n_estimators':[32, 64, 128], 'max_depth':[6, 8, 10]} \n",
    "#increasing estimators increases training time, depth doesn't\n",
    "clf = GridSearchCV(rf, parameters, cv=2, verbose=2) ##made cv =2 to for time\n",
    "clf.fit(doc_term, tweets_df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'n_estimators': 128}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.631787</td>\n",
       "      <td>0.718489</td>\n",
       "      <td>0.582133</td>\n",
       "      <td>0.668063</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 32}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.567313</td>\n",
       "      <td>0.663522</td>\n",
       "      <td>0.596960</td>\n",
       "      <td>0.672603</td>\n",
       "      <td>0.717006</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.343557</td>\n",
       "      <td>0.653107</td>\n",
       "      <td>0.586848</td>\n",
       "      <td>0.714432</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 64}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.597695</td>\n",
       "      <td>0.700734</td>\n",
       "      <td>0.575996</td>\n",
       "      <td>0.728130</td>\n",
       "      <td>0.026863</td>\n",
       "      <td>0.028293</td>\n",
       "      <td>0.010850</td>\n",
       "      <td>0.013698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.213514</td>\n",
       "      <td>0.631247</td>\n",
       "      <td>0.600734</td>\n",
       "      <td>0.770760</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 128}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.607124</td>\n",
       "      <td>0.763103</td>\n",
       "      <td>0.594340</td>\n",
       "      <td>0.778418</td>\n",
       "      <td>0.047577</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.007658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.265015</td>\n",
       "      <td>0.665504</td>\n",
       "      <td>0.591302</td>\n",
       "      <td>0.690596</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 32}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.586171</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.596436</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.024258</td>\n",
       "      <td>0.051562</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.003849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.325753</td>\n",
       "      <td>0.686074</td>\n",
       "      <td>0.601258</td>\n",
       "      <td>0.731990</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 64}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.599267</td>\n",
       "      <td>0.738470</td>\n",
       "      <td>0.603249</td>\n",
       "      <td>0.725511</td>\n",
       "      <td>0.164725</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.006479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.542145</td>\n",
       "      <td>0.664952</td>\n",
       "      <td>0.614095</td>\n",
       "      <td>0.788840</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 128}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.618649</td>\n",
       "      <td>0.790356</td>\n",
       "      <td>0.609539</td>\n",
       "      <td>0.787323</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>0.057341</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.734359</td>\n",
       "      <td>0.657954</td>\n",
       "      <td>0.582657</td>\n",
       "      <td>0.717842</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 32}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.588790</td>\n",
       "      <td>0.720126</td>\n",
       "      <td>0.576520</td>\n",
       "      <td>0.715558</td>\n",
       "      <td>0.040217</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.002284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.961533</td>\n",
       "      <td>0.631927</td>\n",
       "      <td>0.605711</td>\n",
       "      <td>0.746393</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 64}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.598219</td>\n",
       "      <td>0.729560</td>\n",
       "      <td>0.613208</td>\n",
       "      <td>0.763227</td>\n",
       "      <td>0.046703</td>\n",
       "      <td>0.027167</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>0.016834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.830532</td>\n",
       "      <td>0.670731</td>\n",
       "      <td>0.614619</td>\n",
       "      <td>0.800630</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 128}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.612886</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.795705</td>\n",
       "      <td>0.098778</td>\n",
       "      <td>0.056866</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.004925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       4.631787         0.718489         0.582133          0.668063   \n",
       "1       5.343557         0.653107         0.586848          0.714432   \n",
       "2       8.213514         0.631247         0.600734          0.770760   \n",
       "3       4.265015         0.665504         0.591302          0.690596   \n",
       "4       6.325753         0.686074         0.601258          0.731990   \n",
       "5      10.542145         0.664952         0.614095          0.788840   \n",
       "6       4.734359         0.657954         0.582657          0.717842   \n",
       "7       6.961533         0.631927         0.605711          0.746393   \n",
       "8      11.830532         0.670731         0.614619          0.800630   \n",
       "\n",
       "  param_max_depth param_n_estimators                                  params  \\\n",
       "0               6                 32    {'max_depth': 6, 'n_estimators': 32}   \n",
       "1               6                 64    {'max_depth': 6, 'n_estimators': 64}   \n",
       "2               6                128   {'max_depth': 6, 'n_estimators': 128}   \n",
       "3               8                 32    {'max_depth': 8, 'n_estimators': 32}   \n",
       "4               8                 64    {'max_depth': 8, 'n_estimators': 64}   \n",
       "5               8                128   {'max_depth': 8, 'n_estimators': 128}   \n",
       "6              10                 32   {'max_depth': 10, 'n_estimators': 32}   \n",
       "7              10                 64   {'max_depth': 10, 'n_estimators': 64}   \n",
       "8              10                128  {'max_depth': 10, 'n_estimators': 128}   \n",
       "\n",
       "   rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                9           0.567313            0.663522           0.596960   \n",
       "1                7           0.597695            0.700734           0.575996   \n",
       "2                5           0.607124            0.763103           0.594340   \n",
       "3                6           0.586171            0.694444           0.596436   \n",
       "4                4           0.599267            0.738470           0.603249   \n",
       "5                2           0.618649            0.790356           0.609539   \n",
       "6                8           0.588790            0.720126           0.576520   \n",
       "7                3           0.598219            0.729560           0.613208   \n",
       "8                1           0.612886            0.805556           0.616352   \n",
       "\n",
       "   split1_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0            0.672603      0.717006        0.020472        0.014824   \n",
       "1            0.728130      0.026863        0.028293        0.010850   \n",
       "2            0.778418      0.047577        0.021370        0.006392   \n",
       "3            0.686747      0.024258        0.051562        0.005133   \n",
       "4            0.725511      0.164725        0.054358        0.001991   \n",
       "5            0.787323      0.012099        0.057341        0.004555   \n",
       "6            0.715558      0.040217        0.004905        0.006135   \n",
       "7            0.763227      0.046703        0.027167        0.007494   \n",
       "8            0.795705      0.098778        0.056866        0.001733   \n",
       "\n",
       "   std_train_score  \n",
       "0         0.004541  \n",
       "1         0.013698  \n",
       "2         0.007658  \n",
       "3         0.003849  \n",
       "4         0.006479  \n",
       "5         0.001517  \n",
       "6         0.002284  \n",
       "7         0.016834  \n",
       "8         0.004925  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "## the best score we could get out the rf was .61."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6201221528366698, 0.02569524963547301)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this is a process of model selection \n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(nb, doc_term, tweets_df[\"Label\"], cv=5)\n",
    "np.mean(scores), np.std(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.593391754255219, 0.020379661558376658)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lr, doc_term, tweets_df[\"Label\"], cv=5)\n",
    "np.mean(scores), np.std(scores) ##LR is working the best with the data we currently have! \n",
    "\n",
    "#should continue to read about cross-validation \n",
    "# (re: splitting stratetgies, what's the best way to split your data. \n",
    "#you have a bunch of true and false labeled data, \n",
    "#is it important you maintain the same level of true/false in each split?)\n",
    "#jared likes shuffle_split\n",
    "\n",
    "\n",
    "##regularization (penalization) sets up a wall to avoid overfitting. \n",
    "# it's still possible to overfit, with strong evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5973222316449516, 0.021815359336591584)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(rf, doc_term, tweets_df[\"Label\"], cv=5)\n",
    "np.mean(scores), np.std(scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(Xtrain, Ytrain)\n",
    "lr.fit(Xtrain, Ytrain)\n",
    "rf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## each classification model has a predict prob. y_hat = predicted probabiliy\n",
    "# makes a prediction for every tweet not in the training set\n",
    "yhat_nb = nb.predict_proba(Xtest)\n",
    "yhat_lr = lr.predict_proba(Xtest)\n",
    "yhat_rf = rf.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## two competing metrics\n",
    "# specificity (how good you are at false negatives) and sensitivity (how well can identify false positives)\n",
    "# ROC measures how well actually distinguishing between these two classes \n",
    "\n",
    "from sklearn.metrics import roc_auc_score as roc  \n",
    "## takes a list of p's and a list of labels and gives me a number out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99038553, 0.00961447],\n",
       "       [0.01380784, 0.98619216],\n",
       "       [0.02391917, 0.97608083],\n",
       "       ...,\n",
       "       [0.04041444, 0.95958556],\n",
       "       [0.00204746, 0.99795254],\n",
       "       [0.50667901, 0.49332099]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_nb\n",
    "## p 0 (non-ironic) is the first column, 1 (ironic) is second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6792236842105265"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## roc wants the true labels, and then p that it's = to 1, by slicing second column to the array\n",
    "\n",
    "roc(Ytest, yhat_nb[:,1])   ## : means give me every single row. the 1 means \"i want the 1th column\"\n",
    "\n",
    "# worst ROC score = .5, best = 1.0\n",
    "# .5 is the worst because it's a 50/50 chance\n",
    "# ROC score is a probability. \n",
    "# means if you give me an ironic and a non-ironic tweet, 66% of the time i can distinguish them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6975043859649123"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc(Ytest, yhat_lr[:,1])   ###will always need to report the curve with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## to increase the score, need better features, not better model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6465021929824561"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc(Ytest, yhat_rf[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBQrDMAzDJ/8PVjEjw2SGxdsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a442f76d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##fpr and tpr are like x and y values\n",
    "## this figure shows up in every machine learning classifier paper\n",
    "# this curve moves up if you identify true pos. (correctly), \n",
    "# moves to the right if you identify something wrong\n",
    "# worst score = .5 (baseline, 50/50), best score is 1\n",
    "\n",
    "##document-term matrix is not capturing all features of irony well\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_randomforest, tpr_randomforest, _ = roc_curve(Ytest, yhat_rf[:,1]) \n",
    "fpr_naivebayes, tpr_naivebayes, _ = roc_curve(Ytest, yhat_nb[:,1]) \n",
    "fpr_logisticregression, tpr_logisticregression, _ = roc_curve(Ytest, yhat_lr[:,1]) \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_randomforest, tpr_randomforest, lw=2,\n",
    "         label='ROC curve (area = %0.2f)' % roc(Ytest, yhat_rf[:,1]))\n",
    "plt.plot(fpr_naivebayes, tpr_naivebayes, lw=2,\n",
    "         label='ROC curve (area = %0.2f)' % roc(Ytest, yhat_nb[:,1]))\n",
    "plt.plot(fpr_logisticregression, tpr_logisticregression, lw=2,\n",
    "         label='ROC curve (area = %0.2f)' % roc(Ytest, yhat_lr[:,1]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"./roc_curve_baseline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## with n-grams will make a new column for every n-gram vs every word\n",
    "## get my baseline and then get them to work better. \n",
    "## then compare them to the state of the art lstm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## now going to improve baselines with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Introspection/Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## how to get feature importances out of these models?\n",
    "## which words are most important in determining if something is ironic or not?\n",
    "\n",
    "##train a new lr, get weights out, turn from weights to words that are most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jordan_Earnest/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "## need to implement stopwords\n",
    "\n",
    "# cv = CountVectorizer(ngram_range=(1,3), stop_words=\"english\") \n",
    "# doc_term = cv.fit_transform(lst_of_strings)\n",
    "# doc_term = doc_term.todense() \n",
    "\n",
    "## do tf-idf vectorizer\n",
    "cv = TfidfVectorizer(ngram_range=(1,3), stop_words=\"english\") \n",
    "doc_term = cv.fit_transform(lst_of_strings)\n",
    "doc_term = doc_term.todense() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(doc_term, tweets_df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00340052, -0.09638679, -0.09638679, ..., -0.07686238,\n",
       "        -0.07686238, -0.07686238]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_\n",
    "## getting the coefficients of the features\n",
    "## if a coef is large in magnitude, it's important! \n",
    "## that feature has a lot of weight in determinign irony!\n",
    "\n",
    "## each number in the array corresponds to importance of a single word \n",
    "## (or n-gram, depending. in this case ngram = 1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the index of highest weights \n",
    "# np.argsort(lr.coef_)\n",
    "#this would give us the indices from  smallest to largest \n",
    "\n",
    "#however, we want the magnitude (a large negative coef is also import)\n",
    "\n",
    "least_ironic_lst = np.argsort(lr.coef_)[0][:20] \n",
    "most_ironic_lst = np.argsort(lr.coef_)[0][-20:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glad\n",
      "hours\n",
      "funny\n",
      "really\n",
      "waking\n",
      "working\n",
      "monday\n",
      "just love\n",
      "day\n",
      "wait\n",
      "work\n",
      "wow\n",
      "unamused_face\n",
      "nice\n",
      "oh\n",
      "yay\n",
      "thanks\n",
      "fun\n",
      "great\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "for i in most_ironic_lst:\n",
    "    print(cv.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http\n",
      "follow\n",
      "tonight\n",
      "check\n",
      "https\n",
      "think\n",
      "2015\n",
      "understand\n",
      "seen\n",
      "dont\n",
      "literally\n",
      "long\n",
      "need\n",
      "face_throwing_a_kiss\n",
      "rt\n",
      "change\n",
      "idea\n",
      "did\n",
      "girl\n",
      "playing\n"
     ]
    }
   ],
   "source": [
    "for i in least_ironic_lst:\n",
    "    print(cv.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## could sacrifice a lower roc store for a different ngram\n",
    "## do tf-idf vectorizer\n",
    "\n",
    "## in presentation add tweets with most ironic words in them! (just love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## next implement LSTM sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
